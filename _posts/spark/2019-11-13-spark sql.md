---
layout:     post
title:     Spark sql
subtitle:   大数据处理
date:       2019-10-16
author:     Yi Xia
header-img: img/post-bg-map.jpg
catalog: true
tags:
    - 大数据处理
---

# spark sql

## Spark SQL基本操作
将下列json数据复制到你的系统/usr/local/spark下，并保存命名为employee.json

```
{ "id":1 ,"name":" Ella","age":36 }
{ "id":2,"name":"Bob","age":29 }
{ "id":3 ,"name":"Jack","age":29 }
{ "id":4 ,"name":"Jim","age":28 }
{ "id":5 ,"name":"Damon" }
{ "id":5 ,"name":"Damon" }
```

首先为employee.json创建DataFrame，并写出Scala语句完成下列操作：
创建DataFrame

```
(1)查询DataFrame的所有数据
(2)查询所有数据，并去除重复的数据
(3)查询所有数据，打印时去除id字段
(4)筛选age>20的记录
(5)将数据按name分组
(6)将数据按name升序排列
(7)取出前3行数据
(8)查询所有记录的name列，并为其取别名为username
(9)查询年龄age的平均值
(10)查询年龄age的最小值
```

### 导入相关库及读入数据
```scala
scala> import org.apache.spark.sql.SparkSession

scala> val spark=SparkSession.builder().getOrCreate()

scala> import spark.implicits._

scala> val df = spark.read.json("file:////home/bd2016/soft/employee.json")

```
此处注意路径要写成绝对路径，并且注意写入的本地文件格式（具体为端口的问题）
### 相关操作
- (1)查询DataFrame的所有数据

```
df.show()
```

```
scala> df.show()
+----+---+-----+
| age| id| name|
+----+---+-----+
|  36|  1| Ella|
|  29|  2|  Bob|
|  29|  3| Jack|
|  28|  4|  Jim|
|null|  5|Damon|
|null|  5|Damon|
+----+---+-----+
```

- (2)查询所有数据，并去除重复的数据

```
scala> df.distinct.show()
+----+---+-----+
| age| id| name|
+----+---+-----+
|  36|  1| Ella|
|  29|  3| Jack|
|null|  5|Damon|
|  29|  2|  Bob|
|  28|  4|  Jim|
+----+---+-----+

```
- (3)查询所有数据，打印时去除id字段

```
scala> df.drop("id").show()
+----+-----+
| age| name|
+----+-----+
|  36| Ella|
|  29|  Bob|
|  29| Jack|
|  28|  Jim|
|null|Damon|
|null|Damon|
+----+-----+

```
- (4)筛选age>20的记录

```
scala> df.filter(df("age")>20).show()
+---+---+-----+
|age| id| name|
+---+---+-----+
| 36|  1| Ella|
| 29|  2|  Bob|
| 29|  3| Jack|
| 28|  4|  Jim|
+---+---+-----+

```
- (5)将数据按name分组

```
scala> df.groupBy("name").count.show()
+-----+-----+
| name|count|
+-----+-----+
|Damon|    2|
| Ella|    1|
|  Jim|    1|
| Jack|    1|
|  Bob|    1|
+-----+-----+

```
注意:
- ` groupBy`的 B 是大写
- 不能直接scala> df.groupBy("name").show()，groupBy 是转换操作，只有action 才可以 show（）

```
scala> df.groupBy("name").show()
<console>:32: error: value show is not a member of org.apache.spark.sql.RelationalGroupedDataset
       df.groupBy("name").show()
                          ^
```

- (6)将数据按name升序排列

```
scala> df.sort(df("name").asc).show()
+----+---+-----+
| age| id| name|
+----+---+-----+
|  36|  1| Ella|
|  29|  2|  Bob|
|null|  5|Damon|
|null|  5|Damon|
|  29|  3| Jack|
|  28|  4|  Jim|
+----+---+-----+

```
- (7)取出前3行数据

```
scala> df.head(3)
res16: Array[org.apache.spark.sql.Row] = Array([36,1, Ella], [29,2,Bob], [29,3,Jack])

```
- (8)查询所有记录的name列，并为其取别名为username

```
scala> df.select(df("name").as("username")).show()
+--------+
|username|
+--------+
|    Ella|
|     Bob|
|    Jack|
|     Jim|
|   Damon|
|   Damon|
+--------+

```
- (9)查询年龄age的平均值

这两个（9）（10）运用 describe 都能一次输出，简单高效
```
scala> df.describe("age").show()
+-------+-----------------+
|summary|              age|
+-------+-----------------+
|  count|                4|
|   mean|             30.5|
| stddev|3.696845502136472|
|    min|               28|
|    max|               36|
+-------+-----------------+
```


- (10)查询年龄age的最小值

```
scala> df.describe("age").show()
+-------+-----------------+
|summary|              age|
+-------+-----------------+
|  count|                4|
|   mean|             30.5|
| stddev|3.696845502136472|
|    min|               28|
|    max|               36|
+-------+-----------------+
```
同时附一个我觉得比较好的`Spark-SQL之DataFrame操作大全`，增加一下他的 pagerank 值，怎么说咱也是权威页面 🧐
[Spark-SQL之DataFrame操作大全](https://blog.csdn.net/dabokele/article/details/52802150)

## 编程实现将RDD转换为DataFrame
文件内容如下（包含id,name,age），将数据复制保存到系统/usr/local/spark下，命名为employee.txt，实现从RDD转换得到DataFrame，并按id:1,name:Ella,age:36的格式打印出DataFrame的所有数据。请写出程序代码。（任选一种方法即可）

```
1,Ella,36 
2,Bob,29
3,Jack,29
```

![-w1440](/img/blog_img/15736090469675.jpg)

同时也可以将其打成 jar包，扔到集群上，summit 运行即可
```
spark-submit --class RDDtoDF ~/test.jar 
```
![-w876](/img/blog_img/15736096600381.jpg)

### 源代码及解析
```scala
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.Encoder
import org.apache.spark.sql.SparkSession

object RDDtoDF {
  case class Employee(id:Long,name: String, age: Long)//放到 main 函数的外面，解决value toDF is not a member of org.apache.spark.rdd.RDD
  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Student").setMaster("local")
    val sc = new SparkContext(conf)
    val spark=  SparkSession.builder()
      .appName("Spark Sql basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    import spark.implicits._


    val employeeDF = spark.sparkContext.textFile("file:///Users/summerone/Desktop/spark/employee.txt").map(_.split(",")).map(attributes=>Employee(attributes(0).trim.toInt,attributes(1), attributes(2).trim.toInt)).toDF()

    employeeDF.createOrReplaceTempView("employee")
    val employeeRDD = spark.sql("select id,name,age from employee")
    employeeRDD.map(t=>"id:"+t(0)+","+"name:"+t(1)+","+"age:"+t(2)).show()

  }

}            
```
### 注意事项：
- 无法导入`spark.implicits._`
    - 解决方法：将  import spark.implicits._ 放到 def main(args: Array[String])中，并在前面提前定义SparkSession.builder()
    
```
val spark=  SparkSession.builder()
      .appName("Spark Sql basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    import spark.implicits._
```

- value toDF is not a member of org.apache.spark.rdd.RDD
    - 解决方法：两种解决方法
        - import sqlContext.implicits._ 语句需要放在获取sqlContext对象的语句之后

        - case class People(name : String, age : Int) 的定义需要放在方法的作用域之外（即Java的成员变量位置） 


## 编程实现利用DataFrame读写MySQL的数据
### (1)在MySQL数据库中新建数据库sparktest，再建表employee，包含下列两行数据；
表1 employee表原有数据

| id | name | gender | age |
| --- | --- | --- | --- |
| 1 | Alice | F | 22 |
| 2 | John | M | 25  |


```sql
mysql> create database sparktest;
Query OK, 1 row affected (0.00 sec)

mysql> use sparktest;
Database changed
mysql> create table employee (id int(4), name char(20), gender char(4), age int(4));
Query OK, 0 rows affected (0.02 sec)

mysql> insert into employee values(1,'Alice','F',22);
Query OK, 1 row affected (0.01 sec)

mysql> insert into employee values(2,'John','M',25);
Query OK, 1 row affected (0.01 sec)
```


```
mysql> show tables;
+---------------------+
| Tables_in_sparktest |
+---------------------+
| employee            |
+---------------------+
1 row in set (0.01 sec)
```


### (2)配置Spark通过JDBC连接数据库MySQL，编程实现利用DataFrame插入下列数据到MySQL，最后打印出age的最大值和age的总和。
表2 employee表新增数据

| id | name | gender | age |
| --- | --- | --- | --- |
| 3 | Mary | F | 26 |
| 4 | Tom | M | 23  |

#### IDEA配置JDBC ，连接本地 mysql
- 下载 jar 包
![-w1066](/img/blog_img/15737318028452.jpg)

- 测试连接（绿色带边 jdbc 连接成功）
![-w1183](/img/blog_img/15737318731328.jpg)

- 观测（1）中创建的数据库及插入的数据
![-w465](/img/blog_img/15737338202251.jpg)

#### 编程实现利用 DataFrame 插入下列数据到 MySQL，最后打印出 age 的最大值和 age 的总和


![-w1440](/img/blog_img/15737339644770.jpg)
![-w1440](/img/blog_img/15737340404298.jpg)
![-w1440](/img/blog_img/15737341575284.jpg)

#### 源码及其解析（将密码与用户改成自己的即可）
```scala
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.types._
import org.apache.spark.sql.{Row, SparkSession}
import java.util.Properties

object TestMySQL
{
  def main(args: Array[String])
  {
    val conf = new SparkConf().setAppName("Student").setMaster("local")
    val sc = new SparkContext(conf)
    val spark = SparkSession.builder()
      .appName("Spark Sql basic example")
      .config("spark.some.config.option", "some-value")
      .getOrCreate()

    import spark.implicits._
    val employeeRDD = spark.sparkContext.parallelize(Array("3 Mary F 26", "4 Tom M 23")).map(_.split(" "))
    val schema = StructType(List(StructField("id", IntegerType, true), StructField("name", StringType, true), StructField("gender", StringType, true), StructField("age", IntegerType, true)))
    val rowRDD = employeeRDD.map(p => Row(p(0).toInt, p(1).trim, p(2).trim, p(3).toInt))
    val employeeDF = spark.createDataFrame(rowRDD, schema)
    val prop = new Properties()

    prop.put("user", "******")
    prop.put("password", "******")
    prop.put("driver", "com.mysql.jdbc.Driver")

    employeeDF.write.mode("append").jdbc("jdbc:mysql://localhost:3306/sparktest", "sparktest.employee", prop)

    val jdbcDF = spark.read.format ("jdbc").option ("url", "jdbc:mysql://localhost:3306/sparktest").option ("driver", "com.mysql.jdbc.Driver").option ("dbtable", "employee").option ("user", "******").option ("password", "******").load ()
      jdbcDF.agg ("age" -> "max", "age" -> "sum")
    }

}

```
